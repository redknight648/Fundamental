{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvhhC-rTTzd5"
      },
      "source": [
        "NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kykrjqaNJ-C6",
        "outputId": "d80bf3e9-64ed-4fac-dad2-17fab94b981a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCt89N_IKK2I",
        "outputId": "0d96c0f2-05d2-4c14-cf41-b206852503a2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['This',\n",
              " 'is',\n",
              " 'an',\n",
              " 'example',\n",
              " 'sentence',\n",
              " ',',\n",
              " 'showing',\n",
              " 'off',\n",
              " 'the',\n",
              " 'tokenization',\n",
              " 'process',\n",
              " '.']"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "text = \"This is an example sentence, showing off the tokenization process.\"\n",
        "tokens=word_tokenize(text)\n",
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1E_RaaRKs8l",
        "outputId": "3f498fef-d41c-4349-a09e-23bc6b88109a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[('This', 'DT'),\n",
              " ('is', 'VBZ'),\n",
              " ('an', 'DT'),\n",
              " ('example', 'NN'),\n",
              " ('sentence', 'NN'),\n",
              " (',', ','),\n",
              " ('showing', 'VBG'),\n",
              " ('off', 'RP'),\n",
              " ('the', 'DT'),\n",
              " ('tokenization', 'NN'),\n",
              " ('process', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk  import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "pos=pos_tag(tokens)\n",
        "pos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hb6EwVp7L3JT",
        "outputId": "9ec70302-25fb-40d0-c1d3-b3b08eb28b0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting svgling\n",
            "  Downloading svgling-0.4.0-py3-none-any.whl (23 kB)\n",
            "Collecting svgwrite (from svgling)\n",
            "  Downloading svgwrite-1.4.3-py3-none-any.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.1/67.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: svgwrite, svgling\n",
            "Successfully installed svgling-0.4.0 svgwrite-1.4.3\n"
          ]
        }
      ],
      "source": [
        "!pip install svgling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "fSRkv9s4K441",
        "outputId": "5882873a-59b4-4e4d-eb90-990ddcd4adff"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "image/svg+xml": "<svg baseProfile=\"full\" height=\"120px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,656.0,120.0\" width=\"656px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"7.31707%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">This</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"3.65854%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"6.09756%\" x=\"7.31707%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">is</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBZ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"10.3659%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"4.87805%\" x=\"13.4146%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">an</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"15.8537%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"10.9756%\" x=\"18.2927%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">example</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"23.7805%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"12.1951%\" x=\"29.2683%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">sentence</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"35.3659%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"3.65854%\" x=\"41.4634%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">,</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">,</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"43.2927%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"10.9756%\" x=\"45.122%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">showing</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBG</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50.6098%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"6.09756%\" x=\"56.0976%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">off</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">RP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"59.1463%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"6.09756%\" x=\"62.1951%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">the</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"65.2439%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"17.0732%\" x=\"68.2927%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">tokenization</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"76.8293%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"10.9756%\" x=\"85.3659%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">process</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"90.8537%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"3.65854%\" x=\"96.3415%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"98.1707%\" y1=\"19.2px\" y2=\"48px\" /></svg>",
            "text/plain": [
              "Tree('S', [('This', 'DT'), ('is', 'VBZ'), ('an', 'DT'), ('example', 'NN'), ('sentence', 'NN'), (',', ','), ('showing', 'VBG'), ('off', 'RP'), ('the', 'DT'), ('tokenization', 'NN'), ('process', 'NN'), ('.', '.')])"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "from nltk import ne_chunk\n",
        "chunk=ne_chunk(pos)\n",
        "chunk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wh0gcAzgLiYE",
        "outputId": "0febdbdb-f5ca-43ee-f9bb-d0dffef5a269"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'neg': 0.0, 'neu': 0.548, 'pos': 0.452, 'compound': 0.765}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "text = \"NLTK is a great package for working with natural language data.\"\n",
        "\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "scores = analyzer.polarity_scores(text)\n",
        "\n",
        "print(scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cudjwkBeMaHI",
        "outputId": "c12f28a0-011a-47ca-c2f7-3967a77a746c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package snowball_data to /root/nltk_data...\n",
            "[nltk_data]   Package snowball_data is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['nltk',\n",
              " 'is',\n",
              " 'a',\n",
              " 'great',\n",
              " 'packag',\n",
              " 'for',\n",
              " 'work',\n",
              " 'with',\n",
              " 'natur',\n",
              " 'languag',\n",
              " 'data',\n",
              " '.']"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('snowball_data')\n",
        "from nltk.stem import PorterStemmer\n",
        "words=word_tokenize(text)\n",
        "stemmer=PorterStemmer()\n",
        "w=[stemmer.stem(word) for word in words]\n",
        "w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Chhl-ecGRNYG",
        "outputId": "d20c8afa-9adb-4443-b920-222752545a48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "increase\n",
            "play\n",
            "play\n",
            "playing\n",
            "playing\n",
            "playing\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "print(lemmatizer.lemmatize('increases'))\n",
        "print(lemmatizer.lemmatize('playing', pos=\"v\")) #verb\n",
        "print(lemmatizer.lemmatize('playing', pos=\"v\"))\n",
        "print(lemmatizer.lemmatize('playing', pos=\"n\")) #noun\n",
        "print(lemmatizer.lemmatize('playing', pos=\"a\")) #adj\n",
        "print(lemmatizer.lemmatize('playing', pos=\"r\")) #adv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWRNAeCpR9-A",
        "outputId": "4aa2509b-cdd2-43df-ca3c-0b70ca52bb4a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['This',\n",
              " 'sample',\n",
              " 'sentence',\n",
              " ',',\n",
              " 'showing',\n",
              " 'stop',\n",
              " 'words',\n",
              " 'filtration',\n",
              " '.',\n",
              " 'Here',\n",
              " 'write',\n",
              " 'whatever',\n",
              " 'want',\n",
              " '.',\n",
              " 'You',\n",
              " 'also',\n",
              " 'add',\n",
              " 'big',\n",
              " 'text',\n",
              " 'file',\n",
              " 'see',\n",
              " 'technique',\n",
              " 'works']"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "example_sent = \"This is a sample sentence, showing off the stop words filtration. Here you can write whatever you want to. You can also add a very big text file and see how this technique works\"\n",
        "stop_words=set(stopwords.words('english'))\n",
        "word_token=word_tokenize(example_sent)\n",
        "filt=[w for w in word_token if w not in stop_words]\n",
        "filt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oWHV4P5bSoff",
        "outputId": "61e1b1d3-264f-4344-a92e-5d420f0a97fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "T : 1\n",
            "h : 7\n",
            "i : 10\n",
            "s : 11\n",
            "  : 33\n",
            "a : 11\n",
            "m : 1\n",
            "p : 2\n",
            "l : 4\n",
            "e : 17\n",
            "n : 9\n",
            "t : 13\n",
            "c : 4\n",
            ", : 1\n",
            "o : 12\n",
            "w : 7\n",
            "g : 2\n",
            "f : 4\n",
            "r : 7\n",
            "d : 4\n",
            ". : 2\n",
            "H : 1\n",
            "y : 3\n",
            "u : 4\n",
            "v : 2\n",
            "Y : 1\n",
            "b : 1\n",
            "x : 1\n",
            "q : 1\n",
            "k : 1\n"
          ]
        }
      ],
      "source": [
        "freq=nltk.FreqDist(example_sent)\n",
        "for key,value in freq.items():\n",
        "  print(str(key),\":\",str(value))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxutubkjS-v1",
        "outputId": "36d8eb48-144c-407b-b91f-8913963d5787"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This : 1\n",
            "is : 1\n",
            "a : 2\n",
            "sample : 1\n",
            "sentence : 1\n",
            ", : 1\n",
            "showing : 1\n",
            "off : 1\n",
            "the : 1\n",
            "stop : 1\n",
            "words : 1\n",
            "filtration : 1\n",
            ". : 2\n",
            "Here : 1\n",
            "you : 2\n",
            "can : 2\n",
            "write : 1\n",
            "whatever : 1\n",
            "want : 1\n",
            "to : 1\n",
            "You : 1\n",
            "also : 1\n",
            "add : 1\n",
            "very : 1\n",
            "big : 1\n",
            "text : 1\n",
            "file : 1\n",
            "and : 1\n",
            "see : 1\n",
            "how : 1\n",
            "this : 1\n",
            "technique : 1\n",
            "works : 1\n"
          ]
        }
      ],
      "source": [
        "freq=nltk.FreqDist(word_token)\n",
        "for key,value in freq.items():\n",
        "  print(str(key),\":\",str(value))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAg-H0zyTbCP",
        "outputId": "276dad81-dc89-40d9-9564-5e2284af4451"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['dog.n.01', 'frump.n.01', 'dog.n.03', 'cad.n.01', 'frank.n.02', 'pawl.n.01', 'andiron.n.01', 'chase.v.01']\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import wordnet\n",
        "synonym=[]\n",
        "for syns in wordnet.synsets('dog'):\n",
        "  synonym.append(syns.name())\n",
        "print(synonym)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cknfSwwDUEHw"
      },
      "source": [
        "NLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20vULlHLQ3po",
        "outputId": "15612136-4213-41dd-d61e-ff308f0befdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  archive.zip\n",
            "  inflating: Test.csv                \n",
            "  inflating: Train.csv               \n",
            "  inflating: Valid.csv               \n"
          ]
        }
      ],
      "source": [
        "!unzip archive.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pcPvx7BIUK2o"
      },
      "outputs": [],
      "source": [
        "\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgve1xh3hMVx",
        "outputId": "0702d6f1-735b-40e8-e98a-2e7a11ade808"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YE-teX_6hZFV"
      },
      "outputs": [],
      "source": [
        "def preprocess(text):\n",
        "  text=re.sub('<[^>]*>',' ',text).lower()\n",
        "  words = word_tokenize(text)\n",
        "\n",
        "  stop_words=set(stopwords.words('english'))\n",
        "  words=[w for w in words if w not in stop_words]\n",
        "\n",
        "  lemmatizer=WordNetLemmatizer()\n",
        "  words=[lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "  preprocessed_text=' '.join(words)\n",
        "  return preprocessed_text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "train = pd.read_csv('Train.csv')\n",
        "test = pd.read_csv('Test.csv')\n",
        "valid = pd.read_csv('Valid.csv')"
      ],
      "metadata": {
        "id": "qL9b3vKtxUcd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['preprocessed_text'] = train['text'].apply(preprocess)\n",
        "test['preprocessed_text'] = test['text'].apply(preprocess)\n",
        "valid['preprocessed_text'] = valid['text'].apply(preprocess)\n"
      ],
      "metadata": {
        "id": "0BXmitDGxRrC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X_train = train['preprocessed_text']\n",
        "y_train = train['label']\n",
        "X_test = test['preprocessed_text']\n",
        "y_test = test['label']"
      ],
      "metadata": {
        "id": "ng_t9z1tvwzE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Grhbw1igijzp"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "import numpy as np\n",
        "def get_google(data):\n",
        "  model=api.load(\"word2vec-google-news-300\")\n",
        "  tokenized=[sentence.split() for sentence in data]\n",
        "  emb=[]\n",
        "  for sentence in tokenized:\n",
        "    sent_emb=[]\n",
        "    for word in sentence:\n",
        "      if word in model:\n",
        "        sent_emb.append(model[word])\n",
        "    if sent_emb:\n",
        "      emb.append(np.mean(sent_emb,axis=0))\n",
        "    else:\n",
        "      emb.append(np.zeros(300))\n",
        "  return np.array(emb)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train_svm_with_google_word2vec(train_data, test_data):\n",
        "    X_train = get_google(train_data)\n",
        "    X_test = get_google(test_data)\n",
        "\n",
        "    clf = SVC()\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    return y_pred"
      ],
      "metadata": {
        "id": "SzMyofXqva6Z"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# y_pred_google_word2vec = train_svm_with_google_word2vec(X_train, X_test)\n",
        "# accuracy_google_word2vec = accuracy_score(y_test, y_pred_google_word2vec)\n",
        "# print(accuracy_google_word2vec)"
      ],
      "metadata": {
        "id": "F6oQchpCw8PD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN,Dense,Embedding,Bidirectional,LSTM,GRU,Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "max_words=10000\n",
        "max_len=100\n",
        "tokenizer=Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(train['preprocessed_text'])\n",
        "\n",
        "X_train=tokenizer.texts_to_sequences(train['preprocessed_text'])\n",
        "X_test = tokenizer.texts_to_sequences(test['preprocessed_text'])\n",
        "X_valid = tokenizer.texts_to_sequences(valid['preprocessed_text'])\n",
        "\n",
        "X_train = pad_sequences(X_train, maxlen=max_len)\n",
        "X_test = pad_sequences(X_test, maxlen=max_len)\n",
        "X_valid = pad_sequences(X_valid, maxlen=max_len)\n",
        "\n",
        "y_train=to_categorical(train['label'])\n",
        "y_test = to_categorical(test['label'])\n",
        "y_valid = to_categorical(valid['label'])\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, 128, input_length=max_len))\n",
        "model.add(SimpleRNN(64))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "early_stop=EarlyStopping(monitor='val_loss',patience=5,restore_best_weights=True)\n",
        "\n",
        "history=model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=15, batch_size=128)\n",
        "\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "\n",
        "print(\"Test accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sELBq1I0xCtJ",
        "outputId": "88396180-ac78-4be3-d70b-e41f828ebe9e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "313/313 [==============================] - 59s 170ms/step - loss: 0.4407 - accuracy: 0.7872 - val_loss: 0.3209 - val_accuracy: 0.8706\n",
            "Epoch 2/15\n",
            "313/313 [==============================] - 39s 124ms/step - loss: 0.2507 - accuracy: 0.9036 - val_loss: 0.3182 - val_accuracy: 0.8780\n",
            "Epoch 3/15\n",
            "313/313 [==============================] - 35s 111ms/step - loss: 0.1445 - accuracy: 0.9494 - val_loss: 0.4325 - val_accuracy: 0.8652\n",
            "Epoch 4/15\n",
            "313/313 [==============================] - 33s 106ms/step - loss: 0.0614 - accuracy: 0.9795 - val_loss: 0.5234 - val_accuracy: 0.8474\n",
            "Epoch 5/15\n",
            "313/313 [==============================] - 33s 107ms/step - loss: 0.0351 - accuracy: 0.9883 - val_loss: 0.6349 - val_accuracy: 0.8448\n",
            "Epoch 6/15\n",
            "313/313 [==============================] - 32s 103ms/step - loss: 0.0353 - accuracy: 0.9879 - val_loss: 0.7559 - val_accuracy: 0.8638\n",
            "Epoch 7/15\n",
            "313/313 [==============================] - 32s 103ms/step - loss: 0.0343 - accuracy: 0.9884 - val_loss: 0.7266 - val_accuracy: 0.8338\n",
            "Epoch 8/15\n",
            "313/313 [==============================] - 33s 106ms/step - loss: 0.0269 - accuracy: 0.9911 - val_loss: 0.7722 - val_accuracy: 0.8616\n",
            "Epoch 9/15\n",
            "313/313 [==============================] - 32s 101ms/step - loss: 0.0176 - accuracy: 0.9943 - val_loss: 0.7917 - val_accuracy: 0.8452\n",
            "Epoch 10/15\n",
            "313/313 [==============================] - 33s 104ms/step - loss: 0.0133 - accuracy: 0.9958 - val_loss: 0.9227 - val_accuracy: 0.8300\n",
            "Epoch 11/15\n",
            "313/313 [==============================] - 31s 100ms/step - loss: 0.0232 - accuracy: 0.9922 - val_loss: 1.0157 - val_accuracy: 0.7906\n",
            "Epoch 12/15\n",
            "313/313 [==============================] - 32s 102ms/step - loss: 0.0377 - accuracy: 0.9877 - val_loss: 0.7951 - val_accuracy: 0.8358\n",
            "Epoch 13/15\n",
            "313/313 [==============================] - 32s 103ms/step - loss: 0.0284 - accuracy: 0.9902 - val_loss: 0.8954 - val_accuracy: 0.8064\n",
            "Epoch 14/15\n",
            "313/313 [==============================] - 33s 104ms/step - loss: 0.0106 - accuracy: 0.9966 - val_loss: 0.8874 - val_accuracy: 0.8508\n",
            "Epoch 15/15\n",
            "313/313 [==============================] - 33s 105ms/step - loss: 0.0109 - accuracy: 0.9966 - val_loss: 1.0000 - val_accuracy: 0.8032\n",
            "157/157 [==============================] - 2s 10ms/step - loss: 0.9893 - accuracy: 0.8032\n",
            "Test accuracy: 0.8032000064849854\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Sequential()\n",
        "model.add(Embedding(max_words,128,input_length=max_len))\n",
        "model.add(Bidirectional(LSTM(64)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "early_stop=EarlyStopping(monitor='val_loss',patience=5,restore_best_weights=True)\n",
        "\n",
        "history=model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=25, batch_size=128)\n",
        "\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "\n",
        "print(\"Test accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "innuIehG134-",
        "outputId": "353c1e28-7ffa-4fb2-81ec-4e33fa97d9e8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "313/313 [==============================] - 33s 88ms/step - loss: 0.3626 - accuracy: 0.8386 - val_loss: 0.2842 - val_accuracy: 0.8854\n",
            "Epoch 2/25\n",
            "313/313 [==============================] - 12s 39ms/step - loss: 0.2196 - accuracy: 0.9155 - val_loss: 0.2843 - val_accuracy: 0.8862\n",
            "Epoch 3/25\n",
            "313/313 [==============================] - 8s 26ms/step - loss: 0.1695 - accuracy: 0.9363 - val_loss: 0.3193 - val_accuracy: 0.8782\n",
            "Epoch 4/25\n",
            "313/313 [==============================] - 7s 23ms/step - loss: 0.1244 - accuracy: 0.9540 - val_loss: 0.3424 - val_accuracy: 0.8746\n",
            "Epoch 5/25\n",
            "313/313 [==============================] - 6s 19ms/step - loss: 0.0842 - accuracy: 0.9711 - val_loss: 0.4424 - val_accuracy: 0.8682\n",
            "Epoch 6/25\n",
            "313/313 [==============================] - 5s 16ms/step - loss: 0.0655 - accuracy: 0.9771 - val_loss: 0.5633 - val_accuracy: 0.8688\n",
            "Epoch 7/25\n",
            "313/313 [==============================] - 6s 18ms/step - loss: 0.0402 - accuracy: 0.9870 - val_loss: 0.5840 - val_accuracy: 0.8576\n",
            "Epoch 8/25\n",
            "313/313 [==============================] - 5s 16ms/step - loss: 0.0412 - accuracy: 0.9869 - val_loss: 0.6419 - val_accuracy: 0.8690\n",
            "Epoch 9/25\n",
            "313/313 [==============================] - 6s 18ms/step - loss: 0.0351 - accuracy: 0.9893 - val_loss: 0.6472 - val_accuracy: 0.8626\n",
            "Epoch 10/25\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 0.0215 - accuracy: 0.9931 - val_loss: 0.7855 - val_accuracy: 0.8678\n",
            "Epoch 11/25\n",
            "313/313 [==============================] - 5s 17ms/step - loss: 0.0248 - accuracy: 0.9920 - val_loss: 0.8079 - val_accuracy: 0.8622\n",
            "Epoch 12/25\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 0.0149 - accuracy: 0.9955 - val_loss: 0.8455 - val_accuracy: 0.8666\n",
            "Epoch 13/25\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 0.0148 - accuracy: 0.9957 - val_loss: 0.8142 - val_accuracy: 0.8630\n",
            "Epoch 14/25\n",
            "313/313 [==============================] - 5s 17ms/step - loss: 0.0168 - accuracy: 0.9952 - val_loss: 0.9826 - val_accuracy: 0.8668\n",
            "Epoch 15/25\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 0.0112 - accuracy: 0.9970 - val_loss: 0.8943 - val_accuracy: 0.8608\n",
            "Epoch 16/25\n",
            "313/313 [==============================] - 5s 17ms/step - loss: 0.0175 - accuracy: 0.9947 - val_loss: 1.0506 - val_accuracy: 0.8610\n",
            "Epoch 17/25\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 0.0128 - accuracy: 0.9960 - val_loss: 0.9511 - val_accuracy: 0.8656\n",
            "Epoch 18/25\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 0.0089 - accuracy: 0.9977 - val_loss: 0.9170 - val_accuracy: 0.8656\n",
            "Epoch 19/25\n",
            "313/313 [==============================] - 5s 17ms/step - loss: 0.0086 - accuracy: 0.9973 - val_loss: 1.0275 - val_accuracy: 0.8604\n",
            "Epoch 20/25\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 0.0046 - accuracy: 0.9987 - val_loss: 1.1589 - val_accuracy: 0.8644\n",
            "Epoch 21/25\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 0.0088 - accuracy: 0.9973 - val_loss: 1.1346 - val_accuracy: 0.8648\n",
            "Epoch 22/25\n",
            "313/313 [==============================] - 5s 16ms/step - loss: 0.0065 - accuracy: 0.9982 - val_loss: 1.0538 - val_accuracy: 0.8652\n",
            "Epoch 23/25\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 0.0096 - accuracy: 0.9969 - val_loss: 1.1106 - val_accuracy: 0.8674\n",
            "Epoch 24/25\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 0.0115 - accuracy: 0.9961 - val_loss: 0.9956 - val_accuracy: 0.8576\n",
            "Epoch 25/25\n",
            "313/313 [==============================] - 5s 17ms/step - loss: 0.0086 - accuracy: 0.9973 - val_loss: 0.9651 - val_accuracy: 0.8552\n",
            "157/157 [==============================] - 1s 8ms/step - loss: 0.8914 - accuracy: 0.8582\n",
            "Test accuracy: 0.8582000136375427\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=Sequential()\n",
        "model.add(Embedding(max_words,128,input_length=max_len))\n",
        "model.add(GRU(64))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "early_stop=EarlyStopping(monitor='val_loss',patience=5,restore_best_weights=True)\n",
        "\n",
        "history=model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=25, batch_size=128)\n",
        "\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "\n",
        "print(\"Test accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCdc4ZB42Kco",
        "outputId": "fcc0f994-84da-41f2-f09a-1e1203808fb4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "313/313 [==============================] - 26s 75ms/step - loss: 0.3728 - accuracy: 0.8270 - val_loss: 0.2912 - val_accuracy: 0.8818\n",
            "Epoch 2/25\n",
            "313/313 [==============================] - 12s 40ms/step - loss: 0.2268 - accuracy: 0.9126 - val_loss: 0.2884 - val_accuracy: 0.8824\n",
            "Epoch 3/25\n",
            "313/313 [==============================] - 7s 24ms/step - loss: 0.1705 - accuracy: 0.9363 - val_loss: 0.3247 - val_accuracy: 0.8796\n",
            "Epoch 4/25\n",
            "313/313 [==============================] - 5s 17ms/step - loss: 0.1304 - accuracy: 0.9527 - val_loss: 0.3696 - val_accuracy: 0.8786\n",
            "Epoch 5/25\n",
            "313/313 [==============================] - 4s 14ms/step - loss: 0.0954 - accuracy: 0.9670 - val_loss: 0.4018 - val_accuracy: 0.8696\n",
            "Epoch 6/25\n",
            "313/313 [==============================] - 5s 17ms/step - loss: 0.0737 - accuracy: 0.9755 - val_loss: 0.4493 - val_accuracy: 0.8772\n",
            "Epoch 7/25\n",
            "313/313 [==============================] - 4s 13ms/step - loss: 0.0579 - accuracy: 0.9811 - val_loss: 0.5701 - val_accuracy: 0.8726\n",
            "Epoch 8/25\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.0452 - accuracy: 0.9856 - val_loss: 0.6561 - val_accuracy: 0.8682\n",
            "Epoch 9/25\n",
            "313/313 [==============================] - 5s 15ms/step - loss: 0.0328 - accuracy: 0.9899 - val_loss: 0.7157 - val_accuracy: 0.8634\n",
            "Epoch 10/25\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0391 - accuracy: 0.9874 - val_loss: 0.6833 - val_accuracy: 0.8640\n",
            "Epoch 11/25\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0223 - accuracy: 0.9933 - val_loss: 0.8040 - val_accuracy: 0.8534\n",
            "Epoch 12/25\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.0228 - accuracy: 0.9928 - val_loss: 0.7361 - val_accuracy: 0.8610\n",
            "Epoch 13/25\n",
            "313/313 [==============================] - 3s 11ms/step - loss: 0.0184 - accuracy: 0.9944 - val_loss: 0.8186 - val_accuracy: 0.8652\n",
            "Epoch 14/25\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.0152 - accuracy: 0.9955 - val_loss: 0.8961 - val_accuracy: 0.8668\n",
            "Epoch 15/25\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.0138 - accuracy: 0.9963 - val_loss: 0.9241 - val_accuracy: 0.8694\n",
            "Epoch 16/25\n",
            "313/313 [==============================] - 4s 12ms/step - loss: 0.0128 - accuracy: 0.9962 - val_loss: 1.0103 - val_accuracy: 0.8666\n",
            "Epoch 17/25\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0117 - accuracy: 0.9967 - val_loss: 0.9494 - val_accuracy: 0.8678\n",
            "Epoch 18/25\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0113 - accuracy: 0.9966 - val_loss: 0.9771 - val_accuracy: 0.8672\n",
            "Epoch 19/25\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.0123 - accuracy: 0.9959 - val_loss: 1.0813 - val_accuracy: 0.8696\n",
            "Epoch 20/25\n",
            "313/313 [==============================] - 4s 11ms/step - loss: 0.0078 - accuracy: 0.9979 - val_loss: 1.0197 - val_accuracy: 0.8626\n",
            "Epoch 21/25\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0081 - accuracy: 0.9974 - val_loss: 1.1207 - val_accuracy: 0.8676\n",
            "Epoch 22/25\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0054 - accuracy: 0.9985 - val_loss: 1.2094 - val_accuracy: 0.8642\n",
            "Epoch 23/25\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0066 - accuracy: 0.9981 - val_loss: 1.1647 - val_accuracy: 0.8640\n",
            "Epoch 24/25\n",
            "313/313 [==============================] - 4s 11ms/step - loss: 0.0034 - accuracy: 0.9991 - val_loss: 1.1653 - val_accuracy: 0.8628\n",
            "Epoch 25/25\n",
            "313/313 [==============================] - 3s 10ms/step - loss: 0.0138 - accuracy: 0.9958 - val_loss: 1.0731 - val_accuracy: 0.8630\n",
            "157/157 [==============================] - 1s 5ms/step - loss: 1.0225 - accuracy: 0.8674\n",
            "Test accuracy: 0.8673999905586243\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V41w2eDMAD3l"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}